# Paper
## [ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM](https://arxiv.org/abs/2007.11898)
### (20/03/2024 - 23/03/2024)
### Code：

1. ReconstrctWithTwoViews (Tracking.cc)  //对极约束获得两帧相对运动  
--------------------------------------------⬇---------------------------------------  
KannalaBrandt8::ReconstructWithTwoViews (KannalaBrandt8.cpp)  //畸变矫正  
--------------------------------------------⬇---------------------------------------  
[TwoViewReconstruction::Reconstruct (TwoViewReconstruction.cc) // 对极约束求解位姿](https://github.com/zhangx297/ORBSLAM3/blob/master/src/TwoViewReconstruction.cc)


2. ComputeBow
```c
void TemplatedVocabulary<TDescriptor,F>::transform(
  const std::vector<TDescriptor>& features,
  BowVector &v, FeatureVector &fv, int levelsup) const //leverlsup距离叶子的深度
{
  v.clear();
  fv.clear();
  
  if(empty()) // safe for subclasses
  {
    return;
  }
  
  // normalize 
  // 根据选择的评分类型来确定是否需要将BowVector归一化
  LNorm norm;
  bool must = m_scoring_object->mustNormalize(norm);
  
  typename vector<TDescriptor>::const_iterator fit;
  
  if(m_weighting == TF || m_weighting == TF_IDF)
  {
    unsigned int i_feature = 0;
    // 遍历图像中所有的描述子
    for(fit = features.begin(); fit < features.end(); ++fit, ++i_feature)
    {
      WordId id; // 叶子节点的Word id
      NodeId nid; // 父节点
      WordValue w;  // 叶子节点Word对应的权重
      // w is the idf value if TF_IDF, 1 if TF
      
      //  将当前描述子转化为Word id， Word weight，节点所属的父节点id（这里的父节点不是叶子的上一层，它距离叶子深度为levelsup）
      // 调用transform利用orb特征字典将特征点的描述子转换成词袋向量mBowVec以及特征向量mFeatVec
      transform(*fit, id, w, &nid, levelsup);
      
      if(w > 0) // not stopped
      { 
        // 如果Word 权重大于0，将其添加到BowVector 和 FeatureVector
        // 比如，我们一帧中BowVec如下：
        // <2,64><5,75><8,0><16,61><18,45>
        // 我们新到一个一个<2,32>那么我们现在的BowVec为：
        // <2,96><5,75><8,0><16,61><18,45>
        // 比如我们新到一个<9,7>的那么我们现在的BowVec为：
        // <2,96><5,75><8,0><9,7><16,61><18,45>
        v.addWeight(id, w);
        //比如<6,<4,6,7,8>>的含义就是这个帧的第4，6，7，8个特征点在ORB词典中的索引为6的节点下。
        fv.addFeature(nid, i_feature);
      }
    }
    
    if(!v.empty() && !must)
    {
      // unnecessary when normalizing
      const double nd = v.size();
      for(BowVector::iterator vit = v.begin(); vit != v.end(); vit++) 
        vit->second /= nd;
    }
  
  }
  else // IDF || BINARY
  {
    unsigned int i_feature = 0;
    for(fit = features.begin(); fit < features.end(); ++fit, ++i_feature)
    {
      WordId id;
      NodeId nid;
      WordValue w;
      // w is idf if IDF, or 1 if BINARY
      
      transform(*fit, id, w, &nid, levelsup);
      
      if(w > 0) // not stopped
      {
        v.addIfNotExist(id, w);
        fv.addFeature(nid, i_feature);
      }
    }
  } // if m_weighting == ...
  
  if(must) v.normalize(norm);
}
```

### Results analysis

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/ORBSALM3.jpg)

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator](https://arxiv.org/pdf/1708.03852.pdf)
### (22/03/2024 - 23/03/2024)
### Methods

![VINS_1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_1.jpg)

![VINS_2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_2.jpg)

![VINS_3.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_3.jpg)

![VINS_4.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_4.jpg)

![VINS_5.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_5.jpg)

### Results analysis

![2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS-Mono.jpg)

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [A Novel Feature Points Tracking Algorithm in Terms of IMU-Aided Information Fusion](https://ieeexplore.ieee.org/document/9197627)
### (23/03/2024 - 24/03/2024)
### METHODS
1. PREINTEGRATION-BASED PREDICTOR
  1) Initialization

  ![Initialization.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Initialization.jpg)

  2) Pose Prediction：Substitute the pose of a camera into IMU preintegration.
  3) Prediction of Feature Points Position: A linear least-square problem about the Epipolar constraint computes the position of feature points.
3. Local Detector With a Search Window


### Code of ORBSLAM3
1. [Tracking::TrackWithMotionModel // 恒速模型](https://github.com/zhangx297/ORBSLAM3/blob/master/src/Tracking.cc)   
------------------------------⬇------------------------------  
[int ORBmatcher::SearchByProjection // 搜索特征匹配点 //上一帧地图点投影与当前帧进行匹配 //为什么使用imu不优化一下呢？](https://github.com/zhangx297/ORBSLAM3/blob/master/src/ORBmatcher.cc)   

2. LocalMapping::CreateNewMapPoints()  
------------------------------⬇------------------------------  
SearchForTriangulation () // 对极约束约束匹配范围，对满足约束的进行特征匹配(BoW) //对极约束有特征点求位姿，有位姿求特征点

### Paper
#### [Fast Feature Matching in Visual-Inertial](https://ieeexplore.ieee.org/document/10004220)
METHODS  
```c
1. preintegration to estimate pose // 预积分估计位姿
2. if(Point features have not been initialized) 
    utilize Epipolar constraint
   else
    directly project 
3. matching features
```
Question: Feature points that have not BA are directly projected to the current frame in the paper.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------
# Datasets
```
link: https://pan.baidu.com/s/12txr7dfafhbjul-j88esqw  

Extracted code: l17u
```
---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

# Summary
1. Read code and related experiments of ORBSALM3.
2. Finished deriving the formula and results analysis for VINS-Mono.
3. Read IMU-Aided Information Fusion and related papers. 
# Plan 
1. Read papers about feature matching.
2. Write Undergraduate Graduation Project.
3. Coordinated Navigation and Positioning Project
# Problem
1. I do not understand why preintegration updates position and rotation using the pose of a camera when initialization is completed in IMU-Aided Information Fusion.
2. An idea about descriptor randomly choosing feature points. 
