# Paper
## [ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM](https://arxiv.org/abs/2007.11898)
### (20/03/2024 - 23/03/2024)
### Code：

1. ReconstrctWithTwoViews (Tracking.cc)  //对极约束获得两帧相对运动  
--------------------------------------------⬇---------------------------------------  
KannalaBrandt8::ReconstructWithTwoViews (KannalaBrandt8.cpp)  //畸变矫正  
--------------------------------------------⬇---------------------------------------  
[TwoViewReconstruction::Reconstruct (TwoViewReconstruction.cc) // 对极约束求解位姿](https://github.com/zhangx297/ORBSLAM3/blob/master/src/TwoViewReconstruction.cc)


2. ComputeBow
```c
void TemplatedVocabulary<TDescriptor,F>::transform(
  const std::vector<TDescriptor>& features,
  BowVector &v, FeatureVector &fv, int levelsup) const //leverlsup距离叶子的深度
{
  v.clear();
  fv.clear();
  
  if(empty()) // safe for subclasses
  {
    return;
  }
  
  // normalize 
  // 根据选择的评分类型来确定是否需要将BowVector归一化
  LNorm norm;
  bool must = m_scoring_object->mustNormalize(norm);
  
  typename vector<TDescriptor>::const_iterator fit;
  
  if(m_weighting == TF || m_weighting == TF_IDF)
  {
    unsigned int i_feature = 0;
    // 遍历图像中所有的描述子
    for(fit = features.begin(); fit < features.end(); ++fit, ++i_feature)
    {
      WordId id; // 叶子节点的Word id
      NodeId nid; // 父节点
      WordValue w;  // 叶子节点Word对应的权重
      // w is the idf value if TF_IDF, 1 if TF
      
      //  将当前描述子转化为Word id， Word weight，节点所属的父节点id（这里的父节点不是叶子的上一层，它距离叶子深度为levelsup）
      // 调用transform利用orb特征字典将特征点的描述子转换成词袋向量mBowVec以及特征向量mFeatVec
      transform(*fit, id, w, &nid, levelsup);
      
      if(w > 0) // not stopped
      { 
        // 如果Word 权重大于0，将其添加到BowVector 和 FeatureVector
        // 比如，我们一帧中BowVec如下：
        // <2,64><5,75><8,0><16,61><18,45>
        // 我们新到一个一个<2,32>那么我们现在的BowVec为：
        // <2,96><5,75><8,0><16,61><18,45>
        // 比如我们新到一个<9,7>的那么我们现在的BowVec为：
        // <2,96><5,75><8,0><9,7><16,61><18,45>
        v.addWeight(id, w);
        //比如<6,<4,6,7,8>>的含义就是这个帧的第4，6，7，8个特征点在ORB词典中的索引为6的节点下。
        fv.addFeature(nid, i_feature);
      }
    }
    
    if(!v.empty() && !must)
    {
      // unnecessary when normalizing
      const double nd = v.size();
      for(BowVector::iterator vit = v.begin(); vit != v.end(); vit++) 
        vit->second /= nd;
    }
  
  }
  else // IDF || BINARY
  {
    unsigned int i_feature = 0;
    for(fit = features.begin(); fit < features.end(); ++fit, ++i_feature)
    {
      WordId id;
      NodeId nid;
      WordValue w;
      // w is idf if IDF, or 1 if BINARY
      
      transform(*fit, id, w, &nid, levelsup);
      
      if(w > 0) // not stopped
      {
        v.addIfNotExist(id, w);
        fv.addFeature(nid, i_feature);
      }
    }
  } // if m_weighting == ...
  
  if(must) v.normalize(norm);
}
```

### Results analysis

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/ORBSALM3.jpg)

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator](https://arxiv.org/pdf/1708.03852.pdf)
### (22/03/2024 - 23/03/2024)
### Methods

![VINS_1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_1.jpg)

![VINS_2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_2.jpg)

![VINS_3.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_3.jpg)

![VINS_4.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_4.jpg)

![VINS_5.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS_5.jpg)

![VINS_6.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS-mono_6.jpg)

### Results analysis

![2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/VINS-Mono.jpg)

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [A Novel Feature Points Tracking Algorithm in Terms of IMU-Aided Information Fusion](https://ieeexplore.ieee.org/document/9197627)
### (23/03/2024 - 24/03/2024)
### METHODS
1. PREINTEGRATION-BASED PREDICTOR
    1) Initialization

    ![Initialization.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Initialization.jpg)

    2) Pose Prediction：Substitute the pose of a camera into IMU preintegration.
    3) Prediction of Feature Points Position: A linear least-square problem about the Epipolar constraint computes the position of feature points (z, x are unknown).
3. Local Detector With a Search Window


### Code of ORBSLAM3
1. [Tracking::TrackWithMotionModel // 恒速模型](https://github.com/zhangx297/ORBSLAM3/blob/master/src/Tracking.cc)   
------------------------------⬇------------------------------  
[int ORBmatcher::SearchByProjection // 搜索特征匹配点 //上一帧地图点投影与当前帧进行匹配 //为什么使用imu不优化一下呢？](https://github.com/zhangx297/ORBSLAM3/blob/master/src/ORBmatcher.cc)   

2. LocalMapping::CreateNewMapPoints()  
------------------------------⬇------------------------------  
SearchForTriangulation () // 对极约束约束匹配范围，对满足约束的进行特征匹配(BoW) //对极约束有特征点求位姿，有位姿求特征点

### Paper
#### [Fast Feature Matching in Visual-Inertial](https://ieeexplore.ieee.org/document/10004220)
METHODS  
```c
1. preintegration to estimate pose // 预积分估计位姿
2. if(Point features have not been initialized) 
    utilize Epipolar constraint
   else
    directly project 
3. matching features
```

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [* A Tightly Coupled Feature-Based Visual-Inertial Odometry With Stereo Cameras](https://ieeexplore.ieee.org/document/9782093/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. IMU-AIDED FRONT-END PROCESSING
    1) IMU Short-Term Pose Estimation
    2) IMU-Aided Descriptor Construction: Due to large camera motion, perspective deformation occurs between consecutive left image frames taken from the same scene. According to the depth value of the feature, design a novel IMU-aided descriptor based on BRIEF. 
        * model 1: b. Perspective deformation shown in Fig. b is considered to be similar to that in Fig. a and can be ignored. c. Since camera translations in the sliding window are very small relative to the average depth of all the image corners, image scale change can be well approached by the pyramid method.  
        * model 2: def. The depth value of the feature points can be obtained from a stereo camera. According to the depth of a feature, construct a corrected FAST corner.

    ![2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/A%20Tightly%20A%20Coupled%20Feature-Based%20Visual-Inertial%20Odometry%20With%20Stereo%20Cameras_2.png)

    3) Feature Tracking Using IMU-Aided Matching

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/A%20Tightly%20A%20Coupled%20Feature-Based%20Visual-Inertial%20Odometry%20With%20Stereo%20Cameras_1.png)

2. contribution 
Propose IMU-Aided Descriptor Construction.
---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [A Real-Time Stereo Visual-Inertial SLAM System Based on Point-and-Line Features](https://ieeexplore.ieee.org/document/10005022/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Feature Extraction and Matching

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/A%20Real-Time%20Stereo%20Visual-Inertial%20SLAM%20System%20Based%20on%20Point-and-Line%20Features.png)

2. Line Feature Extraction and Matching
    1) Real-Time Detection of Line Features: Some parameters are hidden, and then used LSD to extract line features.
    2) Line Features Combination: Merge overlapping and neighboring line segments by geometric information.
    3) IMU-LK Assisted Line Feature Matching: 

### contribution 
Use Hierarchical Grid Optical Flow Tracker matching feature points and lines.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [* DGM-VINS: Visual–Inertial SLAM for Complex Dynamic Environments With Joint Geometry Feature Extraction and Multiple Object Tracking](https://ieeexplore.ieee.org/document/10138236/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Geometric Feature Extraction Module
    1) Feature Matching and Grid-Based Feature Detection: 
        * Feature points and feature motion are tracked using optical flow and IMU, respectively.
        * To avoid duplication and aggregation in feature detection, masks are set at the center of stable and unstable feature points.
    2) Joint Geometric Dynamic Feature Extraction: Propose to use DBSCAN clustering to determine whether feature points are static or dynamic by combining geometric consistency and epipolar constraints.

    ![2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/DGM-VINS_2.png)
  
2. Temporal Instance Segmentation Module
    1) Temporal Feature-Based Object Tracking: Object center offsets are established by Object center (CenterNet) and priori trajectory heatmap.

    ![3.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/DGM-VINS_3.png)

    2) Instance Segmentation Newwork With Conditional Convolution: Mask Header

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/DGM-VINS_1.png)

### contribution 
Propose DBSCAN to determine whether feature points are static or dynamic.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [Feature- and Distribution-Based LiDAR SLAM With Generalized Feature Representation and Heuristic Nonlinear Optimization](https://ieeexplore.ieee.org/document/9954444)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Lidar Feature-Based Fast Pose Tracking
    1) Environment Surface Reconstruction Priciple and Criteria: Compute the more accurate the surface reconstruction.
    2) Surface Reconstruction on the Longitudinal Slice: Perform interpolating linearly between the LiADR points to approximate the environmental curve.
    3) Feature Selection Based on Smoothness: Filter the raw LiDAR point cloud based on the defined smoothness to generate a less redundant point cloud.
    4) Fast Pose Tracking Based on the Feature Point Cloud

2. Distribution-Based Keyframe-to-Keyframe Matching: The optimization process is to iteratively update the pose to decrease the objective function value.

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Feature-%20and%20Distribution-Based%20LiDAR%20SLAM%20With%20Generalized%20Feature%20Representation%20and%20Heuristic%20Nonlinear%20Optimization.png)

### contribution 
Pose tracking based on the curve

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [Loop-Closure Detection Using Local Relative Orientation Matching](https://ieeexplore.ieee.org/document/9423519/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Feature Extraction Based on SuperPoint Network: Employ a pre-trained SuperPoint network by a self-supervised framework to extract robust
interest points and feature descriptors.
2. Aggregated Selective Match Kernel: Introduce the aggregated selective match kernel framework to compact image representation and effectively compute the similarity between images.
3. Temporal Constraint:
### LOOP CLOSING VERIFICATION BASED ON LRO
1. Locality Preserving Matching: Filter out the outliers and construct accurate correspondences accordingly.
2. Relative Orientation of Topological Structures: Each angle formed between the center feature point and its neighbors should be kept
consistent after image transformation.
3. The Affine-Invariant of Topological Structures:

![2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Loop-Closure%20Detection%20Using%20Local%20Relative%20Orientation%20Matching.png)

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [Loop Closure Detection With Bidirectional Manifold Representation Consensus](https://ieeexplore.ieee.org/document/9994239/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Feature Matching With BMRC: 
    1) Intrinsic Representation of Neighborhood: A sample is defined as the K-nearest neighbors in the image planes under the Euclidean metric.
    2) Bidirectional Manifold Representation Consensus: Measure the difference of local geometric structures in our preliminary version.

    ![q.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Loop%20Closure%20Detection%20With%20Bidirectional%20Manifold%20Representation%20Consensus.png)

    3) Iterative Filtering Strategy: Introduce an element-based iterative filtering strategy to acquire a relatively clean set of matches for neighborhood manifold representation.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [Multi-Modal Features and Accurate Place Recognition with Robust Optimization for Lidar-Visual-Inertial SLAM](https://ieeexplore.ieee.org/document/10445759/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Multi-modal Feature Processing
    1) The Coarse Matching Phase: The IMU measurements provide an initial pose transformation between two consecutive frames, and then encode these geometric properties into descriptors for the line segments and select candidate matches.
    2) The Fine Matching Phase: Obtain the optical flow points and measure their distances to each candidate line segment in the image, identifying the closest line segment.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [IGICP: Intensity and Geometry Enhanced LiDAR Odometry](https://ieeexplore.ieee.org/document/10328702/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. An Overview of Intensity and Geometry Based Similarity Matching: Use a two-step point matching strategy by first finding match candidates via KNN search and then determining the final correspondence by feature matching.
2. Geometric Similarity SG of a Point Pair: Adopt the normal vector as one of the features, and then solve the questions about the unbalanced density in different directions. 

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/IGICP%20Intensity%20and%20Geometry%20Enhanced%20LiDAR%20Odometry.png)

3. Intensity Similarity SI: Use symmetric KL-divergence for a local subset.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [Rein-SLAM: Narrow the Gaps Between the Matching Task and SLAM System](https://ieeexplore.ieee.org/document/9942940/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Probabilistic Keypoint Selection
2. Probabilistic Feature Matching

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [A LiDAR SLAM With PCA-Based Feature Extraction and Two-Stage Matching](https://ieeexplore.ieee.org/document/9729241/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Feature Extraction: Distinguish Edge points and Planar points according to the ratio of PCA eigenvalues.

![q.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/A%20LiDAR%20SLAM%20With%20PCA-Based%20Feature%20Extraction%20and%20Two-Stage%20Matching.png)

2. Initial Pose Estimation and Frame Tracking
    1) Initial Pose Estimation: The constant velocity motion model or a plane-to-plane matching method
    2) Frame-to-Sparse Local Map Tracking: Find features with two nearest neighbors, and then compute the distance between features and their corresponding edge.
3. Keyframe Maintenance: Use keyframe in both frame-tracking and mapping modules to get locally accurate and globally consistent estimation results without losing efficiency.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [A LiDAR SLAM System With Geometry Feature Group-Based Stable Feature Selection and Three-Stage Loop Closure Optimization](https://ieeexplore.ieee.org/document/10174668/)
### (25/03/2024 - 27/03/2024)
### METHODS
1. Geometry Feature Extraction:
    1) Downsampling and Neighborhood Estimation: Due to the dense distribution of short-distance ground points, more points are removed with a larger proportion than nonground points. Downsampled large distance ground points query neighborhood points in polar coordinates for higher quality.
    2) Geometry Feature Extraction and Clustering: Use PCA to analyze the geometry characteristics of feature points. Feature points that are close in geometry characteristics and location will be expressed as a group.
    3) Stable Geometry Feature Selection
    4) Adaptive Parameters Adjustment

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

# Datasets
```
link: https://pan.baidu.com/s/12txr7dfafhbjul-j88esqw  

Extracted code: l17u
```
---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

# Summary
1. Read code and related experiments of ORBSALM3.
2. Finished deriving the formula and results analysis for VINS-Mono.
3. Read IMU-Aided Information Fusion and related papers. 
# Plan 
1. Read papers about feature matching.
2. Write Undergraduate Graduation Project.
3. Coordinated Navigation and Positioning Project
# Problem
1. An idea about descriptors randomly choosing feature points. 
