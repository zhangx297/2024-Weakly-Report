# Paper
## [Fast Sequence-Matching Enhanced Viewpoint-Invariant 3-D Place Recognition](https://ieeexplore.ieee.org/document/9351776)
### (01/04/2024 - 02/04/2024)
### METHODS
1. Multilayer Spherical Generation: To apply the feature extraction in the SphereVLAD, need first transform 3-D point clouds into spherical representations.
2. SphereVLAD
    1) Viewpoint-Invariant Feature Extraction: SphereVLAD first extracts the orientation-equivalent features with the spherical convolution operation.
    2) Learning Metrics
3. Fast Matching: 
    1) Particle Initialization: The particles are generated uniformly within the reference descriptors, where each particle represents a potential match.
    2) Particle and Map Updating: Evaluate particle corresponding matching score by following the SeqSLAM procedure. The particles will converge to potential matching targets, and then determine whether to change the sequence resolution level by evaluating an active coverage
    score.
    3) Complexity Analysis

    ![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Fast%20Sequence-Matching%20Enhanced%20Viewpoint-Invariant%203-D%20Place%20Recognition.png)

## [BASL-AD SLAM: A Robust Deep-Learning Feature-Based Visual SLAM System With Adaptive Motion Model](https://ieeexplore.ieee.org/document/10475131)
### (01/04/2024 - 02/04/2024)
### METHODS
1. Feature Extraction: ASL-Feat detects key-points and generates descriptors simultaneously. Add a binary activation layer on the
top of ASL-Feat. 
2. Motion Model
    * Adaptive Model: Propose to compute in parallel three initial camera poses according to the constant velocity model, sliding window average velocity motion, and constant acceleration motion respectively.

## [TextSLAM: Visual SLAM With Semantic Planar Text Features](https://ieeexplore.ieee.org/document/10285400)
### (02/04/2024 - 03/04/2024)
### METHODS
1. SEMANTIC TEXT FEATURES
    1) Geometric Model: Choose an observation model that measures the difference between the detected text object and the projection of the estimated 3D text object on the image.
    2) Semantic Information Management: Represent the semantic information of a text object by two parts: its meaning s and a semantic cost g.

    ![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/TextSLAM.png)

2. TEXTSLAM SYSTEM
    1) Initialization of Text Objects: Adopt AttentionOCR as the text extractor in our implementation. To initialize the parameters θ of a text object newly detected in a keyframe, we track the FAST feature points within the text region by Kanade-Lucas-Tomasi tracker until the next keyframe. 
    2) Camera Pose Estimation With Text Objects: Select text objects observed by previous 2 keyframes for pose estimation and exclude those behind the camera or whose orientation is perpendicular to the current viewing direction.

## [MCOV-SLAM: A Multicamera Omnidirectional Visual SLAM System](https://ieeexplore.ieee.org/document/10404075)
### (02/04/2024 - 03/04/2024)
### METHODS
1. MULTICAMERA OMNIDIRECTIONAL SLAM METHOD
    1) Feature Matching Improvement: When tracking the local map, restrict the angle between camera’s translation direction and observation direction, and eliminate the matching point pairs whose directions mentioned above are approximately parallel, which do not meet the observability conditions of the system.
    2) Keyframe Decision Method: Propose an indicator called observability degree that can quantitatively evaluate the observability of multicamera SLAM system and determine if there is a new keyframe needed.
        * More than 10 frames have been processed after SLAM’s initialization or relocalization.
        * More than 10 frames have been processed since the last keyframe insertion.
        * After SLAM is initialized, the current frame’s observability degree is less than 75% of the maximum observability degree corresponding to all historical frames after the last keyframe.
    3) Omnidirectional Loop-Closing Method: Propose an innovative loop-closing approach that can significantly enrich opportunities for the SLAM system to integrate global constraints and conduct global optimization, consequently enhancing the system’s overall performance.
        
    ![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/MCOV-SLAM.png)  
        
## [Appearance-Based Loop Closure Detection via Locality-Driven Accurate Motion Field Learning](https://ieeexplore.ieee.org/document/9457132)
### (03/04/2024 - 04/04/2024)
### METHODS
1. Feature Extraction Based on Key.Net and HardNet
2. Candidate Frame Selection via ASMK: To formulate the similarity between two images, most popular methods can be summed up as formula of descriptors.
3. Loop Closure Verification Based on LAL: Exploit the nature of motion field in the LCD scene and formulate the feature matching task as a motion field learning problem.
    1) Motion Field Formulation:The goal is to learn a vector-valued function f, or recover a dense motion field. Adopt a conventional way, i.e., using the regularization theory.
    2) Bayesian Framework for Mismatch Removal: To fit the inliers in the putative set, we adopt a mixed model to estimate the motion field f.
    3) Accurate Motion Field Introduced by Locality-Driven Mechanism: Introduce a locality-driven mechanism, which can provide an extra constraint of the motion field and then enhance the accuracy of motion field learning.
        * Constraint of neighboring points: Take xi and its corresponding local support x as starting points, depicting their trajectories from the query to the candidate frame to construct the motion field. 
        * Constraint of neighboring clusters: Exploit a weighted distance d(pi, pj) introduced in [57] to measure the difference among putative matches. φ(·) is a distance measurement function, such as Euclidean and Gaussian kernel distance.

## [A Novel Texture-Less Object Oriented Visual SLAM System](https://ieeexplore.ieee.org/document/8901152)
### (03/04/2024 - 04/04/2024)
### METHODS
1. VISUAL SLAM BASED ON KEYOBJECTS
    1) Feature Extraction: Point features are detected using FAST and described using ORB. Object features are detected using random forest classification techniques. Use rasterized model to describe keyobjects in VSLAM system which can be easily retrieved from object CAD model. 

    ![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/A%20Novel%20Texture-Less%20Object%20Oriented%20Visual%20SLAM%20System.png)

2. KEYOBJECT PARAMETERIZATION
    1) Object Rasterization
    2) Object Pose Estimation
    3) Object Matching
        * Downsample the point group ψ to form a new raster point group q ∈ χ, by default we randomly take one point from three raster points.
        * Every object candidate can that cut across is examined by the following three criteria: a): Whether the candidate and O have same object type; b) Whether the cross points fall on same edge; c) Whether the pixel intensity SSD error between q and the cross points group fall within certain range.

## [Region Aware Video Object Segmentation With Deep Motion Modeling](https://ieeexplore.ieee.org/document/10486830)
### (05/04/2024 - 06/04/2024)
### METHODS
1. Feature Extraction: Adopt ResNet-50 and ResNet-18 as the image and mask encoders.
2. Object Motion Tracker: Employ object position features in previous frames to predict instantaneous motion functions for multi-object tracking.
3. Motion Path Memory: MPM generates the motion path of each object between two frames and then memorizes features within the united motion
paths to filter redundant context.

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Region%20Aware%20Video%20Object%20Segmentation%20With%20Deep%20Motion%20Modeling.png)

4. Memory Propagation: Perform regional matching using L2 similarity to compute the affinity.

## [Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval](https://ieeexplore.ieee.org/document/9437331)
### (06/04/2024 - 07/04/2024)
### METHODS
1. Unimodal Embedding
    1) Visual Embedding: Utilize the CNN pretrained on the ImageNet dataset and transfer its parameters to this task. 
    2) Sentence Embedding: Considering the temporal information in the sentence, the paper use a bidirectional GRU to model the sentence and sequentially input the embedding words into the bidirectional GRU.
    3) Keywords Embedding: Considering that keywords information is a supplement to sentence information, the paper make the keyword embedding matrix and sentence embedding matrix share the parameter matrix.
2. Multiscale Visual Self-Attention:  
    1) First use a multiscale feature fusion network to extract the joint information of each layer.
    2) Use a redundant target filtering network to adaptively filter the insignificant targets and extract the salient features of the image (L2 and 1x1 convolution).
3. Visual-Guided Multimodal Fusion: 
    1) Visual-Guided Attention: Soft Attention, Fusion Attention, Similarity Attention

    ![2.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Exploring%20a%20Fine-Grained%20Multiscale%20Method%20for%20Cross-Modal%20Remote%20Sensing%20Image%20Retrieval_2.png)

    2) Dynamic Fusion of Multimodal Information: Design a dynamic fusion method of multimodal information to enable sentence features and keywords features can fuse dynamically and effectively.

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Exploring%20a%20Fine-Grained%20Multiscale%20Method%20for%20Cross-Modal%20Remote%20Sensing%20Image%20Retrieval_1.png)

## [Aerial Image Stitching Using IMU Data from a UAV](https://ieeexplore.ieee.org/document/10269879)
### (06/04/2024 - 07/04/2024)
### METHODS
1. The Proposed Aerial Image Stitching Algorithm: the integration process accumulates errors, known as drift, due to the presence of noise in the acquired data. To tackle this issue, numerical methods like the Trapezoidal rule and Simpson's rule are crucial.
    ![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Aerial%20Image%20Stitching%20Using%20IMU%20Data%20from%20a%20UAV.png)
    1) Altitude Difference Effect: When the UAV changes altitude between two consecutive image captures, the scale of the images change. USe to rescale the images so that they have the same scale.
    2) Yaw Axis Motion Effect: Use the yaw angle measured by the UAV's IMU to calculate the rotation required.
    3) Pitch Axis Motion Effect: Use the pitch anxgle measured by the UAV's IMU to calculate the transformation required. 

## [Probabilistic Spatial Distribution Prior Based Attentional Keypoints Matching Network](https://ieeexplore.ieee.org/document/9386055)
### (06/04/2024 - 07/04/2024)
### METHODS

![1.jpg](https://github.com/zhangx297/2024-Weekly-Report/blob/main/Pictures%20of%20papers/Probabilistic%20Spatial%20Distribution%20Prior%20Based%20Attentional%20Keypoints%20Matching%20Network.png)

1. Spatial Distribution Prior Module
    1) IMU Integration
    2) Keypoints Prior Module
2. Attentional GNN for Keypoints Matching
    1) Position Encoder: An MLP module is adopted as the position encoder for each feature.
    2) Attentional Layers: Optimizes each keypoint feature f iteratively to obtain the contextual information inand-cross images.
    3) Linear Projection
3. Prior Assisted Attentional GNN for Keypoints Matching
    1) Direct Spatial Prior Integration: A straightforward approach to integrating the prior into attentional GNN is using it to weigh the attention.
    2) Probabilistic Spatial Prior Integration: To integrate the spatial prior into the attention module in a natural way, the paper exploit the probabilistic perspective of the attention.

## [HTSR-VIO: Real-Time Line-Based Visual-Inertial Odometry With Point-Line Hybrid Tracking and Structural Regularity]()
### (07/04/2024 - 08/04/2024)
### PROPOSED POINT-LINE HYBRID TRACKING ALGORITHM
1. Point-Line Feature Extraction
2. Hybrid Tracking Algorithm: Point features around a line feature in the artificial environment have similar geometric characteristics to this line feature. Compute the velocities of point features on the image plane to predict their neighbor line feature.
3. Pose Estimation Refined by LPB
### STRUCTURAL LINE CLUSTER AND RESIDUAL
1. Vanishing Point Update
2. Vanishing Point Measurement Residual
3. . Spatial Consistency of Vertical Line: Use the gravity vector measured by IMU to separate the set of vertical lines.

## [Balancing the Budget: Feature Selection and Tracking for Multi-Camera Visual-Inertial Odometry]()
### (07/04/2024 - 08/04/2024)
### METHODS
1. CROSS CAMERA FEATURE TRACKING
2. SUBMATRIX FEATURE SELECTION: Multi-camera systems increases computation, eventually reaching a point where the algorithm can fail due to computational constraints. Thus, it is important to track and optimize only the best features.
    1) Construct Joint Feature Jacobian Matrix
    2) Submatrix Selection: Since the overall pose error depends on the spectral properties of Jacobian matrix , we aim to find a submatrix Hsub (i.e., subset of features) that best preserves this distribution.

# IMU-Aided feature tracker
## [Code](https://github.com/zhangx297/IMU-Aided-feature-tracker/tree/main/src/IMU-aided_feature_tracking)

# Summary
1. Read papers about feature matching. 
2. Write the IMU-aided codes relying on VINS-Mono.
# Plan 
1. Read papers about feature matching.
2. Write the IMU-aided codes relying on VINS-Mono and ORBSLAM3.
# Problem
1. After camera initialization, IMU-Aided prediction of pose for too long can lead to drift in the scene.
# Idea
1. By increasing pose prediction accuracy, reduce local feature matching radius.
2. Propose an IMU-based descriptor for eliminating fuzzy features.
3. The problem of large disparity
4. The problem of multiscale
5. IMU-Aided prediction of pose for too long can lead to drift in the scene.
