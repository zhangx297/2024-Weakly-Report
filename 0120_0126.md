# Paper
## [On-Manifold Preintegration for Real-Time Visual--Inertial Odometry](https://ieeexplore.ieee.org/document/7557075)
### (22/01/2024 - 23/01/2024)
### Introduction && Related word
1. Introduce cameras, inertial sensors and the existing literatureon VIO imposing a tradeoff between the accuracy and computational efficiency.
2. Related work on VIO can be sectioned along three main dimensions:
    1) The number of camera poses involved in the estimation:  
        * Filtering
        * Fixed-Lag Smoothing
        * Full Smoothing
    2) The representation of the uncertainty for the measurements and the Gaussian priors (EKF)
    3) Distinguishes existing approaches by looking at the number of times in which the measurement model is linearized. 
### Methods
1. PRELIMINARIES:
    1) Notions of Riemannian Geometry: the Special Orthogonal Group and the Special Euclidean Group (Omit)

    ![3.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_3.png)  

    2) Uncertainty Description in SO(3): The integral of the Gaussian distribution compute R˜ (Perturbation Model???).
    3) Gauss–Newton Method on Manifold: A standard Gauss–Newton method works by repeatedly optimizing a quadratic approximation of the objective function using perturbation models.
2. MAP VISUAL–INERTIAL STATE ESTIMATION  
    1) State: x = [R, p, v, b]; X = {x}
    2) Measurements: Z = {C, I}; C ∈ camera, I ∈ IMU

    ![MAP VISUAL–INERTIAL STATE ESTIMATION](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_1.png)  

    3) Factor Graphs and MAP(maximum a posteriori) Estimation:

    ![1.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_4.png.jpg)  

3. IMU MODEL AND MOTION INTEGRATION

![2.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_5.jpg)  

4. IMU PREINTEGRATION ON MANIFOLD

    ![IMU PREINTEGRATION ON MANIFOLD](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_2.png)  

    1) Preintegrated IMU Measurements

    ![3.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_6.jpg)  

    ![3_1.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_7.jpg)  

    2) Noise Propagation

    ![4.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_8.jpg)  

    3) Incorporating Bias Updates

    ![5.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_9.jpg)  

    4) Preintegrated IMU Factors
    
    ![6.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_10.jpg)  

    5) Bias Model  
    5. STRUCTURELESS VISION FACTORS

    ![7.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_11.jpg)  

    ![8.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/On-Manifold%20Preintegration%20for%20Real-Time%20Visual--Inertial%20Odometry_12.jpg)    

### contribution 
1. Proposed a novel preintegration theory, which Improved integration in a global frame.
2. Adopted a structureless model (BA) for visual measurements which avoids optimizing over 3-D landmarks.
### Advantages and Disadvantages
1. More accurate than the state-of-the-art alternatives
### Problem
Unclear about the role of "Uncertainty Description in SO(3)" above

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [A Novel Feature Points Tracking Algorithm in Terms of IMU-Aided Information Fusion](https://ieeexplore.ieee.org/document/9197627)
### (24/01/2024 - 25/01/2024)
### Introduction && Related word
1. Proposed source of IMU: To make the scale observable as well as roll and pitch angle, the low-cost inertial measurement unit (IMU), which can measure triaxial acceleration and angular velocity, is employed to assist the camera in a SLAM task.
2. Early works about feature matching, descriptors and IMU  
    1) IMU: the loosely coupled one and the tightly coupled one:
        * Loose: Fuse the pose estimation at the state variable level by using the extended Kalman filtering (EKF) algorithm.
        * Tight: Use the intrinsic relationship between the visual sensor and IMU to estimate the pose by jointly optimizing the constraints of the visual and IMU measurements, which is more flexible and can be based on both EKF and graph optimization.
### Methods
1. Initialization: Obtain the velocity and gravity vectors which will be used as the initial values for the subsequent iterative process of relative motion estimation.
    1) Strucfture From Motion
    2) IMU Preintegration
    3) Information Fusion → g，v
2. Pose Prediction: Predict the pose of the current frame by using the pose of previous frame and the IMU measurements between these two frames.
3. Prediction of Feature Points Position.
4. Local Detector With a Search Window: 
    1) Proposed different sizes of Search Window for feature matching, which can avoid brute force searching.

    ![1.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/A%20Novel%20Feature%20Points%20Tracking%20Algorithm%20in%20Terms%20of%20IMU-Aided%20Information%20Fusion_2.png)  

    2) Proposed a feature update method to prevent biological population by removing some feature points of converge.

    ![2.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/A%20Novel%20Feature%20Points%20Tracking%20Algorithm%20in%20Terms%20of%20IMU-Aided%20Information%20Fusion_3.png)  

    3) Employed to guide the generation of new features.
    
![3.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/A%20Novel%20Feature%20Points%20Tracking%20Algorithm%20in%20Terms%20of%20IMU-Aided%20Information%20Fusion_1.png)  

### contribution
1. Proposed a new feature points tracking algorithm in terms of IMU-aid information fusion.
2. Proposed novel the local matcher and feature update module
### Advantages and Disadvantages
1. A superior performance in accuracy and efficiency 
### Problem
Does not understand Prediction of Feature Points Position.

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

## [An overview of simultaneous localisation and mapping  towards multi-sensor fusion](https://www.tandfonline.com/doi/abs/10.1080/00207721.2023.2282409)
### (25/01/2024 - 26/01/2024)
### Introduction && Related word
1. Application fields of SLAM technique
2. single senor: The ability of perception is usually restricted. The accuracy and robustness are noticeably decreased particularly when the sensing platform undergoes violent motions.
    1) VSLAM:
        * advantages: rich environmental texture information and achieve positioning and map construction at lower costs
        * disadvantages: scale drift, overexposure and low-light environments
    2) LSLAM:
        * advantages: high-quality positioning and mapping performance in large-scale scenes
        * disadvantages: drift in narrow corridors or large open scenes
3. multi-sensor fusion: more accurate environmental maps, improving the robustness of SLAM systems
4. Formulation and architecture for SLAM
    1) Front end: initial pose estimation
    2) Back end: maximum posterior (MAP)

    ![1.png](https://github.com/zhangx297/2024-Weakly-Report/blob/main/Pictures%20of%20papers/An%20overview%20of%20simultaneous%20localisation%20and%20mapping%20towards%20multi-sensor%20fusion_1.png)

### Methods  
1. Visual-inertial SLAM systems: achieve good positioning and mapping accuracy, while still being lightweight and cost-effective.
    1) loosely: the preintegration technique
    2) tightly: 
        * Filter-based:  
            `Mourikis and Roumeliotis (2007) on the Multi-State Constraint Kalman Filter (MSCKF).`  

            `Extended Kalman Filter (EKF) enables the real-time performance of MSCKF.`  

            `Another representative EKF-based Visual-inertial odometry system is the ROVIO proposed by Bloesch et al. (2015)`  

            `Jackson et al. (2019) extended the ROVIO framework to enhance its robustness through improved dynamic models, keyframe reset, and partial updates.`  

            `Brossard et al. (2018) built a VINS system based on the UKF on Lie Groups (UKF-LG).`  

            `Loianno et al. (2016) also developed a VIO system based on the UKF acting on the Lie group.`  

            `Some research aims to adapt VI-SLAM systems to more types of visual sensors.`  
            `Many systems integrate constraints constructed from various forms of features into several filter frameworks.`  

            `Some systems attempt to improve VISLAM systems without using any geometric features but only by fusing the intensity values of images with IMU measurements (Hardt-Stremayr & Weiss, 2019).`  

            `Some VI-SLAM systems improve the system performance based on analyzing the observability (Hesch et al., 2014a; Huang et al., 2014; Jones & Soatto, 2011) or consistency (Hesch et al., 2014b).`  

        * Optimisation-based:   
            `Visual odometry system based on keyframes and bundle adjustment was OKVIS (Leutenegger et al., 2013).`  

            `Leutenegger (2022) proposed OKVIS2, which can support loop closure and dynamic object removal.`  

            `Mur-Artal and Tardos (2017) introduced ORB-SLAM-VI, a VI-SLAM system capable of zero-drift localisation by reusing the global map. `  

            `The ORB-SLAM3 (Campos et al., 2021) extended ORB-SLAM-VI to support more types of cameras such as RGB-D and stereo cameras.`  

            `Rosinol et al. (2020) introduced Kimera, a VI-SLAM system that leverages a pose graph to estimate global trajectory and allows for mesh reconstruction and semantic labelling in 3D. `  

            `Von Stumberg et al. (2018) developed a monocular visual-inertial odometry system DSO-VI.`  

            `Von Stumberg and Cremers (2022) further extended to DM-VIO based on the local bundle adjustment technique. `  

            `Qin et al. (2018) presented a visual-inertial state estimator VINS-Mono.` 
             
            `A 4-DOF pose graph optimisation is used to reduce drifts and achieve global consistency. `  

            `VINS-Fusion is a further development of VINS-Mono that supports stereo cameras and GPS measurements.`  

            `The optimisation-based VI-SLAM systems are widely developed for various types of cameras.`  

            `The filter-based VI-SLAM systems, many optimisation-based VI-SLAM systems utilise various forms of features to improve the system performance.`

            `Some systems optimise system states by utilising image intensity and IMU measurements (Liu et al., 2021b; Jiang et al., 2020).`  

            `Ding et al. (2021) improved the system performance by adding dynamics and external force constraints in the optimisation.`  

            `Many VISLAM systems are aimed to improve the VI-SLAM systems by increasing the accuracy of optimisation or reducing the computational cost.`

            `Liu et al. (2018) solved the global consistency problem by minimising the re-projection and inertial constraint function once loop closure is found.` 

            `Jaekel et al. (2020) utilised the fixed-lag smoother technique to optimise states in the back end.`  

            `Indelman et al. (2013) integrated an incremental inference algorithm based on the factor graph to reduce computational costs.`  

            `Xiao et al. (2020) presented visual-inertial odometry for the micro air vehicle (MAV) based on nonlinear optimisation.`  

            `Nagy et al. (2020) utilised embedded graphics processing units (GPUs)`  

            `Fan et al. (2021) presented a visualinertial mapping system that can build a dense map.`  

2. Lidar-inertial SLAM systems: Lidar sensors can provide real-scale geometric information of objects, which enables to achieve higher accuracy in loop-closure-free environments than visual-inertial systems.
    1) loosely:  
        `Zhang and Singh (2014) developed the first 3D LISLAM system (LOAM) that integrates mechanical lidar and IMU measurements.`  

        `Shan and Englot (2018) expanded the LOAM and created a Lego-LOAM framework. `  

        `Tang et al. (2015) loosely coupled an inertial navigation system and a lidar SLAM into an EKF framework for UGV applications.`  

        `Zhang et al. (2021) developed a coarse-to-fine loosely-coupled lidar-inertial odometry (LIO) system for urban positioning and mapping.` 

        `Xie et al. (2018) combined lidar scans and IMU data to build a 2.5D occupied grid map based on the Gmapping algorithm.`  

        `Opromolla et al. (2016) presented a lidar-inertial localisation and mapping system for micro Unmanned Aerial Vehicles (MAVs) that fly in 2D environments`  

        `Wu et al. (2021) proposed a lidar-inertial localisation system for UGVs.`  

        `Tagliabue et al. (2021) presented a lidar-inertial observability-aware algorithm (LION) based on the fixed-lag smoother technique`  
    2) tightly:
        * Filter-based  
            `Xu and Zhang (2021) proposed Fast-LIO based on iterated extended Kalman filter.`
            
            `Zhu et al. (2022) proposed a lidar-inertial system initialisation method named LIInit.`  
            
            `Xu et al. (2022b) proposed Fast-LIO2.`  

            `He et al. (2023) developed point-LIO.`  
            
            `Qin et al. (2020) developed a lidar-inertial state estimator named LINS based on an iterated errorstate Kalman filter (ESKF).`  

        * Optimisation-based  
            `Chen et al. (2020) proposed an optimisationbased LIO system for autonomous vehicles in parking lots.`  
            
            `Ye and Liu (2017) developed an LIO system based on non-linear optimisation.`   
            
            `Ye et al. (2019) presented a tightly coupled lidar-IMU odometry system (LIO-MAPPING).`  
            
            `Shan et al. (2020) proposed a tightlycoupled lidar-inertial odometry via smoothing and mapping (LIO-SAM).`  
            
            `Liu and Ou (2022) proposed a lightweight LI-SLAM system based on LEGO-LOAM and LIO-SAM. `  
            
            `Geneva et al. (2018) presented the lidar-inertial 3D plane SLAM (LIPS).`  
            
            `Zhang et al. (2022) proposed a tightly coupled LIO system with plane constraints for the multi-story building.`  
            
            `Li et al. (2020a) presented an LIO system named LiLi-OM for both solid-state and mechanical lidars.`  
            
            `Lv et al. (2021) proposed a lidar inertial system, named CLINS.`  
            
            `Chen et al. (2022) presented a coarseto-fine, lightweight LIO algorithm named DLIO. `  
            
            `Koide et al. (2022) presented an LIO system based on global matching cost minimisation.`  
            
            `Gentil et al. (2019a) employed upsampled preintegrated measurements (UPMs) to mitigate the effects of motion distortion in lidar scans.`  
            
            `Gentil et al. (2019b) advanced their work by introducing the IN2LAAMA system.`  
            
            `Ding et al. (2020) developed an LIO system towards changing city scenes.`  
            
            `Peng et al. (2022) proposed a robust lidar-based localisation (ROLL) system.`  
3. Lidar-visual SLAM systems  
    `Zhang and Singh (2015) presented a framework for loosely combining visual odometry and lidar odometry named V-LOAM.`  
    
    `Zhang et al. (2017) presented the DEMO algorithm that can use depth information from RGB-D cameras and lidar sensors to augment visual odometry systems.`  
    
    `Zhu et al. (2021) developed the CamVox system by fusing Livox lidar measurements into the visual SLAM system ORB-SLAM2.`  
    
    `Graeter et al. (2018) proposed a lidar-monocular visual odometry (LIMO) based on estimating depth from lidar for visual features by fitting local planes.`  
    
    `Shin et al. (2018, 2019) proposed a direct visuallidar SLAM system (DVL-SLAM) that combines the sparse depth information of the lidar with a monocular camera.`  
    
    `Wang et al. (2022a) presented an edgebased infrared-lidar SLAM system (EIL-SLAM).`  
    
    `Huang et al. (2020) presented a lidar-monocular visual odometry using point and line features.`  
    
    `Radmanesh et al. (2020) proposed a lidar and visual localisation and mapping (LIV-LAM) system.`  
    
    `Qian et al. (2022) proposed a visual-lidar SLAM System for UAVs.`  
    
    `Li et al. (2021) proposed a self-supervised visual-lidar odometry (Self-VLO) system.`  
4. Lidar-visual-inertial SLAM systems  
    `Zuo et al. (2019a) proposed a tightly-coupled multi-sensor fusion algorithm named lidar-inertial-camera fusion (LIC-Fusion).`  
    
    `Zuo et al. (2020) proposed LICFusion 2.0. A lidar plane-feature tracking method based on a sliding window is proposed to accelerate the system.`  
    
    `Zheng et al. (2022) proposed a fast and tightlycoupled sparse-direct lidar-inertial-visual odometry (FAST-LIVO).`  
    
    `Zhang et al. (2023) proposed a fully tightly coupled lidar-visual-inertial odometry (FT-LVIO) based on an error-state-iterated Kalman filter.`  
    
    `Shan et al. (2021) proposed a system termed tightly-coupled lidar-visual-inertial odometry via smoothing and mapping (LVI-SAM).`  
    
    `He et al. (2022) proposed a lidar-visual-inertial odometry based on optimised visual point-line features. `  
    
    `Jia et al. (2021) proposed LVIOFusion that can fuse stereo camera, lidar, and IMU into a graph-based optimisation.`  
    
    `Wisth et al. (2021) proposed a multi-sensor odometry system for mobile platforms that jointly optimises visual, lidar, and inertial measurements based on fixed lag smoothing.`  
    
    ` Wisth et al. (2023) further extend this work and proposed visual, inertial, lidar, and leg odometry (VILENS) for all-terrain-legged robots.`  
    
    `Wang et al. (2021) proposed an IMU-centric lidar-camera-inertial mapping and localisation system (MetroLoc) for metro vehicles.`  
    
    `Zhao et al. (2021) proposed an IMU-centric multi-sensor fusion system named Super Odometry that can estimate motions from coarse to fine.`  
    
    `Lin et al. (2021) proposed a robust, realtime multi-sensor fusion framework (R2LIVE).`  
    
    `Lin and Zhang (2022) presented R3LIVE. R3LIVE can reconstruct the RGBcolored 3D dense maps.`  
    
    `Wang et al. (2019b) presented a visual-inertial-lidar SLAM system.`  
    
    `Shao et al. (2019) proposed a stereo visual-inertial-lidar SLAM system (VIL-SLAM).`  
    
    `Khattak et al. (2020a) utilised a VIO system and thermal-inertial odometry (TIO) system to provide initial poses for the lidar sensor, and the lidar odometry builds the environmental map.`  
5. Other multi-sensor fusion SLAM systems
    1) GNSS/GPS: Provide absolute pose information and have the advantage of small accumulated errors.  
        `Yu et al. (2019) proposed a general GPSaided omnidirectional visual-inertial state estimator based on a sliding window.`  
        
        `He et al. (2020) proposed an integrated GNSS/LiDAR-SLAM pose estimation framework in large-scale scenes.`  
        
        `Cao et al. (2021) proposed an EKF-based GNSS-visual-inertial fusion system named GVINS for state estimation.`  
        
        `Tang et al. (2022) proposed an INS-centric GNSS-visualinertial odometry system named IC-GVINS.`  
        
        `Aboutaleb et al. (2020) proposed a filter-based multi-sensor fusion odometry that can loosely couple lidar, IMU, and GNSS measurements.`  
        
        `Zheng et al. (2019) proposed present a GPS-assisted LIO system for autonomous vehicles.`  
        
        `Chiang et al. (2020) proposed an IMU-aiding lidar SLAM system.`  
        
        `Liu et al. (2021a) proposed a GNSS-aided VI-SLAM system.`  
        
        `Han et al. (2022) presented an optimisation-based GPS-visual-inertial odometry for long-term tasks.`  
        
        `Liang et al. (2020) proposed present a multi-sensor fusion framework based on an error-state extended Kalman filter (ESEKF) that can fuse IMU, GNSS, radar, camera, and lidar sensor measurements.`  
    2) Range sensors:   
        `Xu et al. (2020) presented a decentralised relative state estimation framework that fuses camera, inertial, and UWB sensors.`  
        
        `Nguyen et al. (2021) fused the UWB sensor and proposed a range-focused odometry system. `  
        
        `Nguyen et al. (2022a) presented a visual-inertial-range odometry system (VIRO) for collaborative exploration.`  
        
        `Nguyen et al. (2022b) further extended their work as VIRALFusion`  
        
        `Yang et al. (2021) proposed a resilient ultrawideband visual-inertial indoor localisation system (R-UVIS).`  
        
        `Xu et al. (2021) presented an aerial swarm system (Omni-swarm) with decentralised omnidirectional visual-inertial-UWB state estimation.`  
        
        `Cao and Beltrame (2020) proposed the VIR-SLAM system that fused visual, inertial, and range sensor measurements for single and multi-robot systems.`  
        
        `Almalioglu et al. (2020) proposed an ego-motion estimation system named Milli-RIO.`  
        
        `Delaune et al. (2021) presented a range-visual-inertial odometry system named xVIO.`  
        
        `Liu et al. (2020) proposed a synthetic aperture radar (SAR) visual-inertial odometry (SAR-VIO).`  
    3) encoder sensors  
        `Su et al. (2021) proposed an odometry system named GR-LOAM by fusing lidar, IMU, and encoder measurements in a tightly coupled scheme.`  
        
        `Jiang et al. (2021) presented a PIW-SLAM system that tightly coupled the panoramic camera, IMU, and wheel encoder measurements.`  
        
        `Lee et al. (2020) proposed a visual-inertial-wheel odometry (VIWO) system for ground vehicles.`  
    4) magnetic sensors  
        `Lee et al. (2019) proposed a magneto-inertial integrated navigation system used in non-line-of-sight environments.`  
        
        `Wang et al. (2019a) proposed a compass-aided VIO system.`

|Model                                 |Sensor configuration|Tight/loose System structure|Estimation strategy|Loop closing|Open source|  
|:------------------------------------:|:------------------:|:--------------------------:|:-----------------:|:----------:|:---------:|   
|MSF-EKF (Lynen et al., 2013)          |    Camera + IMU    |             Loose          |        EKF        |     No     |     No    |
|MSCKF (Mourikis & Roumeliotis, 2007)  |    Camera + IMU    |             Tight          |        EKF        |     No     |     Yes   |
|OKVIS (Leutenegger et al., 2013)      |    Camera + IMU    |             Tight          |   Optimisation    |     No     |     Yes   |
|ROVIO (Bloesch et al., 2015, 2017)    |    Camera + IMU    |             Tight          |        EKF        |     No     |     Yes   |
|VINS-Fusion (Qin et al., 2018)        |    Camera + IMU    |             Tight          |   Optimisation    |     Yes    |     Yes   |
|DM-VIO (Von Stumberg & Cremers, 2022) |    Camera + IMU    |             Tight          |   Optimisation    |     No     |     Yes   |
|VI-DSO (Von Stumberg et al., 2018)    |    Camera + IMU    |             Tight          |   Optimisation    |     No     |     No    |
|BASALT (Usenko et al., 2020)          |    Camera + IMU    |             Tight          |   Optimisation    |     No     |     Yes   |
|Kimera (Rosinol et al., 2020)         |    Camera + IMU    |             Tight          |   Optimisation    |     Yes    |     Yes   |
|ORB-SLAM3 (Campos et al., 2021)       |    Camera + IMU    |             Tight          |   Optimisation    |     Yes    |     Yes   |
|LOAM (Zhang & Singh, 2014)            |    Lidar + IMU     |             Loose          |   Optimisation    |     No     |     Yes   |
|Lego-LOAM (Shan & Englot, 2018)       |    Lidar + IMU     |             Loose          |   Optimisation    |     Yes    |     Yes   |
|LIO-SAM (Shan et al., 2020)           |    Lidar + IMU     |             Tight          |   Optimisation    |     Yes    |     Yes   |
|LIO-MAPPING (Ye et al., 2019)         |    Lidar + IMU     |             Tight          |   Optimisation    |     Yes    |     Yes   |
|LION (Tagliabue et al., 2021)         |    Lidar + IMU     |             Loose          |   Optimisation    |     No     |     No    |
|LINS (Qin et al., 2020)               |    Lidar + IMU     |             Tight          |        EKF        |     No     |     Yes   |
|DLIO (Chen et al., 2022)              |    Lidar + IMU     |             Loose          |        GICP       |     No     |     No    |
|FAST-LIO2 (Xu et al., 2022b)          |    Lidar + IMU     |             Tight          |        EKF        |     No     |     Yes   |
|DEMO (Zhang et al., 2017)             |    Lidar + Camera  |             Tight          |   Optimisation    |     No     |     Yes   |
|V-LOAM (Zhang & Singh, 2015)          |    Lidar + Camera  |             Loose          |   Optimisation    |     No     |     No    |
|DVL-SLAM (Shin et al., 2018)          |    Lidar + Camera  |             Tight          |   Optimisation    |     Yes    |     Yes   |
|LIMO (Graeter et al., 2018)           |    Lidar + Camera  |             Tight          |   Optimisation    |     No     |     Yes   |
|LIC-Fusion (Zuo et al., 2019a)        |Lidar + Camera + IMU|             Tight          |        EKF        |     No     |     No    |
|LIC-Fusion 2.0 (Zuo et al., 2020)     |Lidar + Camera + IMU|             Tight          |        EKF        |     No     |     No    |
|LVI-SAM (Shan et al., 2021)           |Lidar + Camera + IMU|             Tight          |   Optimisation    |     Yes    |     Yes   |
|R2LIVE (Lin et al., 2021)             |Lidar + Camera + IMU|             Tight          |        EKF        |     No     |     Yes   |
|R3LIVE (Lin & Zhang, 2022)            |Lidar + Camera + IMU|             Tight          |        EKF        |     No     |     Yes   |
|VILENS (Wisth et al., 2023)           |Lidar + Camera + IMU|             Tight          |   Optimisation    |     No     |     No    |
|GVINS (Cao et al., 2021)              |GNSS + Camera + IMU |             Tight          |        EKF        |     No     |     No    |
|R-UVIS (Yang et al., 2021)            | UWB + Camera + IMU |             Tight          |   Optimisation    |     No     |     No    |
|PIW-SLAM (Jiang et al., 2021)         |Encoder+Camera + IMU|             Tight          |   Optimisation    |     No     |     No    |

---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------
# Summary
1. Complete two unfinished papers for derivation formula and experimental analysis last week.
2. Read "An overview of simultaneous localisation and mapping  towards multi-sensor fusion".

# Plan
1. Read "Inertial preintegration for VI-SLAM by the screw motion theory"
2. Read "A Survey on 3D Gaussian Splatting"
3. Internship resignation

# Problem
1. Problems in the above Papers.
2. Relationship between 3D reconstruction rendering(3D Gaussian and NeRF) and SLAM